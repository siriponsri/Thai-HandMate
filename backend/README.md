# ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ LLM Backend - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏µ‡∏° No-Code

**Person 4: ‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö LLM ‡πÅ‡∏•‡∏∞ Backend API**  
‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏û‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏õ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM ‡∏à‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö

## ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö LLM Backend

LLM Backend ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:
- ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å AI Models (Hand A, Hand B, Face API)
- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ (Natural Language Response)
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (Context Analysis) ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå
- API Endpoints ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Frontend

### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ

1. **‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥** - ‡πÉ‡∏ä‡πâ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å Hand + Face
2. **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ** - ‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
3. **Context-Aware** - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
4. **API Integration** - ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Frontend ‡∏Å‡∏±‡∏ö AI Models

---

## ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM Backend

### ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç

```
thai-handmate/backend/
‚îú‚îÄ‚îÄ .env                        ‚Üê ‡πÑ‡∏ü‡∏•‡πå API Keys (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà)
‚îú‚îÄ‚îÄ app.py                      ‚Üê FastAPI application
‚îú‚îÄ‚îÄ requirements.txt            ‚Üê Python dependencies  
‚îú‚îÄ‚îÄ README.md                   ‚Üê ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ô‡∏µ‡πâ
‚îî‚îÄ‚îÄ api_demo.ipynb             ‚Üê ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏î‡∏™‡∏≠‡∏ö
```

### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå .env
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `.env` ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå `backend/` ‡πÅ‡∏•‡∏∞‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:

```bash
# Thai-HandMate Backend Configuration

# Google Gemini API (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ - ‡∏ü‡∏£‡∏µ)
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_API_BASE=https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent

# OpenAI API (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å - ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡πà‡∏≤‡∏¢)
OPENAI_API_KEY=your_openai_api_key_here  
OPENAI_API_BASE=https://api.openai.com/v1/chat/completions

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
LLM_PROVIDER=gemini
CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173
DEBUG=true
```

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Ç‡∏≠ API Key

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Google Gemini (‡∏ü‡∏£‡∏µ)**:
1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://aistudio.google.com/app/apikey
2. ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Google Account
3. ‡∏Ñ‡∏•‡∏¥‡∏Å "Create API key"
4. ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å API key ‡∏°‡∏≤‡πÉ‡∏™‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenAI (‡∏ï‡πâ‡∏≠‡∏á‡∏à‡πà‡∏≤‡∏¢)**:
1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://platform.openai.com/api-keys
2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏á‡∏¥‡∏ô
3. ‡∏™‡∏£‡πâ‡∏≤‡∏á API key ‡πÉ‡∏´‡∏°‡πà
4. ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏™‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env

### ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ

**‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç**: `backend/app.py`

```python
# ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ CORS ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
app.add_middleware(
    CORSMiddleware,
    allow_origins=os.getenv("CORS_ORIGINS", "http://localhost:5173").split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ‡∏õ‡∏£‡∏±‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
@app.post("/generate-sentence")
async def generate_sentence(request: SentenceRequest):
    try:
        # ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Frontend
        hand_words = request.words           # ‡∏à‡∏≤‡∏Å Hand A + Hand B
        face_emotion = request.emotion       # ‡∏à‡∏≤‡∏Å Face API
        context = request.context           # ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM
        prompt = f"""
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:
        
        ‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠: {', '.join(hand_words)}
        ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏à‡∏≤‡∏Å‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤: {face_emotion}
        ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: {context}
        
        ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ:
        1. ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤
        2. ‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ
        3. ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
        4. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß 1-2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        5. ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
        
        ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
        ‡∏Ñ‡∏≥: ["‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏™‡∏ß‡∏¢", "‡∏¢‡∏¥‡πâ‡∏°"] ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå: "happy" 
        -> "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏ß‡∏¢‡∏°‡∏≤‡∏Å‡πÅ‡∏•‡∏∞‡∏¢‡∏¥‡πâ‡∏°‡πÅ‡∏¢‡πâ‡∏°‡πÅ‡∏à‡πà‡∏°‡πÉ‡∏™"
        """
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM API
        response = await call_llm_api(prompt)
        
        return {
            "success": True,
            "sentence": response.strip(),
            "words_used": hand_words,
            "emotion": face_emotion
        }
        
    except Exception as e:
        logger.error(f"Error generating sentence: {e}")
        return {
            "success": False,
            "error": str(e),
            "fallback": f"‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥: {', '.join(request.words)}"
        }
```

**‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç**: `src/components/CameraFeed.jsx` (Frontend)

```javascript
// ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Backend API
const [generatedSentence, setGeneratedSentence] = useState('');

const generateSentence = async () => {
  try {
    const response = await fetch('http://localhost:8000/generate-sentence', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        words: [handAResult.word, handBResult.word].filter(Boolean),
        emotion: faceResult.emotion,
        context: '‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ'
      })
    });
    
    const data = await response.json();
    
    if (data.success) {
      setGeneratedSentence(data.sentence);
    } else {
      setGeneratedSentence(data.fallback || '‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î');
    }
  } catch (error) {
    console.error('Backend API Error:', error);
    setGeneratedSentence('‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Backend ‡πÑ‡∏î‡πâ');
  }
};

// ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó
useEffect(() => {
  if (handAResult && handBResult && faceResult) {
    generateSentence();
  }
}, [handAResult, handBResult, faceResult]);
```

### ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö Backend

1. **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies**:
```bash
cd backend
pip install -r requirements.txt
```

2. **‡∏£‡∏±‡∏ô Backend server**:
```bash
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

3. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö API**:
   - API Documentation: http://localhost:8000/docs
   - Health Check: http://localhost:8000/health

### ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤

**‡∏ñ‡πâ‡∏≤ API Key ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô**:
1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå .env ‡∏°‡∏µ API key ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö quota ‡πÅ‡∏•‡∏∞ billing (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenAI)
3. ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å Gemini ‡πÄ‡∏õ‡πá‡∏ô OpenAI ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏•‡∏±‡∏ö‡∏Å‡∏±‡∏ô

**‡∏ñ‡πâ‡∏≤ CORS Error**:
1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CORS_ORIGINS ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env
2. ‡πÄ‡∏û‡∏¥‡πà‡∏° Frontend URL ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ
3. ‡∏£‡∏µ‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó Backend server

**‡∏ñ‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥**:
1. ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á prompt ‡πÉ‡∏ô app.py
2. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
3. ‡∏õ‡∏£‡∏±‡∏ö temperature parameter

**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢**: LLM ‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡πÑ‡∏î‡πâ < 3 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ, ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏ã‡∏•‡∏π‡∏ä‡∏±‡∏ô LLM

‡πÄ‡∏£‡∏≤‡∏°‡∏µ 3 ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏•‡∏±‡∏Å:

**‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å A: OpenAI GPT (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ - ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)**
- ‚≠ê ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
- üí∞ ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡πà‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢ (~$0.002/1K tokens)
- üîë ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ API Key
- ‚úÖ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Production

**‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å B: Google Gemini (‡∏ü‡∏£‡∏µ - ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ)**
- üÜì ‡∏ü‡∏£‡∏µ (‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
- üîë ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ API Key (‡∏ü‡∏£‡∏µ)
- ‚úÖ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö
- üìö ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡∏î‡∏µ

**‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å C: Local LLM (‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà)**
- üÜì ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢
- üíª ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏π‡∏á
- üîß ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á

**üëâ ‡πÄ‡∏£‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Google Gemini ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡∏ü‡∏£‡∏µ)**

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏Ç‡∏≠ API Key

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Google Gemini (‡∏ü‡∏£‡∏µ):**

1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà <https://aistudio.google.com/app/apikey>
2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ Google ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
3. ‡∏Ñ‡∏•‡∏¥‡∏Å **"Create API Key"**
4. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **"Create API key in new project"**
5. ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å API Key ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenAI (‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô):**

1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà <https://platform.openai.com/api-keys>
2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ OpenAI
3. ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏ö‡∏±‡∏ç‡∏ä‡∏µ (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ $5)
4. ‡∏™‡∏£‡πâ‡∏≤‡∏á API Key

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Environment

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `.env` ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå `backend/`:

```bash
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Google Gemini
GOOGLE_API_KEY=your_gemini_api_key_here
LLM_PROVIDER=gemini

# ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenAI (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏ô‡πÉ‡∏î‡∏≠‡∏±‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á)
# OPENAI_API_KEY=your_openai_api_key_here
# LLM_PROVIDER=openai

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÜ
MIN_CONFIDENCE_THRESHOLD=0.60
MAX_RESPONSE_LENGTH=200
LANGUAGE=th
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Python Libraries

‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå `backend/`:

```bash
pip install google-generativeai openai python-dotenv fastapi uvicorn
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 5: ‡∏™‡∏£‡πâ‡∏≤‡∏á LLM Service

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `backend/services/llm_service.py`:

```python
import os
from dotenv import load_dotenv
import google.generativeai as genai
from openai import OpenAI
from typing import Dict, List, Optional

load_dotenv()

class LLMService:
    def __init__(self):
        self.provider = os.getenv('LLM_PROVIDER', 'gemini')
        self.language = os.getenv('LANGUAGE', 'th')
        
        if self.provider == 'gemini':
            genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
            self.model = genai.GenerativeModel('gemini-pro')
            print("‚úÖ ‡πÉ‡∏ä‡πâ Google Gemini")
            
        elif self.provider == 'openai':
            self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            print("‚úÖ ‡πÉ‡∏ä‡πâ OpenAI GPT")
            
        else:
            raise ValueError("‚ùå LLM Provider ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
    
    def create_context_prompt(self, results: Dict) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå AI"""
        
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Assistant ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÑ‡∏ó‡∏¢

‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:
"""
        
        # Hand A Results
        if 'handA' in results:
            hand_a = results['handA']
            prompt += f"- ‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠ A: {hand_a['prediction']} (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: {hand_a['confidence']:.1%})\\n"
        
        # Hand B Results  
        if 'handB' in results:
            hand_b = results['handB']
            prompt += f"- ‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠ B: {hand_b['prediction']} (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: {hand_b['confidence']:.1%})\\n"
        
        # Face Emotion Results
        if 'face' in results:
            face = results['face']
            if face['detected']:
                prompt += f"- ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏à‡∏≤‡∏Å‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤: {face['emotion']} (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: {face['confidence']:.1%})\\n"
        
        prompt += f\"\"\"
‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÇ‡∏î‡∏¢:
1. ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£
2. ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ï‡πà‡∏≠‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
3. ‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
4. ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
5. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 100 ‡∏Ñ‡∏≥
\"\"\"
        
        return prompt
    
    async def generate_response(self, results: Dict) -> Dict:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏à‡∏≤‡∏Å LLM"""
        
        try:
            prompt = self.create_context_prompt(results)
            
            if self.provider == 'gemini':
                response = self.model.generate_content(prompt)
                text = response.text
                
            elif self.provider == 'openai':
                response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=150,
                    temperature=0.7
                )
                text = response.choices[0].message.content
            
            return {
                'success': True,
                'response': text.strip(),
                'provider': self.provider
            }
            
        except Exception as e:
            print(f"‚ùå LLM Error: {e}")
            return {
                'success': False,
                'response': self._get_fallback_response(results),
                'error': str(e)
            }
    
    def _get_fallback_response(self, results: Dict) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠ LLM ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
        
        if 'handA' in results and results['handA']['confidence'] > 0.6:
            word = results['handA']['prediction']
            return f"‡∏â‡∏±‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥‡∏ó‡πà‡∏≤ '{word}' ‡∏Ñ‡πà‡∏∞"
            
        elif 'handB' in results and results['handB']['confidence'] > 0.6:
            word = results['handB']['prediction'] 
            return f"‡∏â‡∏±‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á '{word}' ‡πÉ‡∏ä‡πà‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞"
            
        else:
            return "‡∏Ç‡∏≠‡∏≠‡∏¥‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏≥‡∏ó‡πà‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞"

# ‡∏™‡∏£‡πâ‡∏≤‡∏á instance
llm_service = LLMService()
```

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 6: ‡πÄ‡∏û‡∏¥‡πà‡∏° API Endpoints

‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå `backend/app.py`:

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Dict, Optional
from services.llm_service import llm_service

app = FastAPI(title="Thai Handmate API")

class PredictionRequest(BaseModel):
    handA: Optional[Dict] = None
    handB: Optional[Dict] = None 
    face: Optional[Dict] = None

class PredictionResponse(BaseModel):
    message: str
    confidence: float
    llm_response: str
    provider: str

@app.post("/api/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    try:
        # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å AI Models
        results = {}
        best_confidence = 0.0
        best_prediction = "unknown"
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Hand A
        if request.handA and request.handA.get('confidence', 0) > 0.6:
            results['handA'] = request.handA
            if request.handA['confidence'] > best_confidence:
                best_confidence = request.handA['confidence'] 
                best_prediction = request.handA['prediction']
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Hand B
        if request.handB and request.handB.get('confidence', 0) > 0.6:
            results['handB'] = request.handB
            if request.handB['confidence'] > best_confidence:
                best_confidence = request.handB['confidence']
                best_prediction = request.handB['prediction']
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Face
        if request.face:
            results['face'] = request.face
        
        # ‡∏™‡πà‡∏á‡πÑ‡∏õ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á
        llm_result = await llm_service.generate_response(results)
        
        return PredictionResponse(
            message=best_prediction,
            confidence=best_confidence,
            llm_response=llm_result['response'],
            provider=llm_result.get('provider', 'fallback')
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "service": "Thai Handmate API"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## üîß ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á

### ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô Backend Server

```bash
cd backend
python app.py
```

Server ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà <http://localhost:8000>

### ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö API

```bash
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Health Check
curl http://localhost:8000/api/health

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Prediction
curl -X POST "http://localhost:8000/api/predict" \\
     -H "Content-Type: application/json" \\
     -d '{
       "handA": {"prediction": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "confidence": 0.85},
       "face": {"detected": true, "emotion": "happy", "confidence": 0.75}
     }'
```

### ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á LLM

‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô `.env`:

```bash
# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
MAX_RESPONSE_LENGTH=150

# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
MIN_CONFIDENCE_THRESHOLD=0.70

# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏†‡∏≤‡∏©‡∏≤
LANGUAGE=en  # ‡∏´‡∏£‡∏∑‡∏≠ th
```

---

## üéØ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û

### 1. Context Enhancement

‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt:

```python
def create_advanced_context_prompt(self, results: Dict, history: List = None) -> str:
    prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÑ‡∏ó‡∏¢

‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô:
"""
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
    if history:
        prompt += "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:\\n"
        for msg in history[-3:]:  # ‡πÄ‡∏≠‡∏≤ 3 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            prompt += f"- {msg}\\n"
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå
    if 'handA' in results and 'handB' in results:
        prompt += "‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏†‡∏≤‡§∑‡∏≤‡∏°‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á 2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•\\n"
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå
    if 'face' in results and results['face']['detected']:
        emotion = results['face']['emotion']
        prompt += f"‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå: ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏î‡∏π{emotion}\\n"
    
    return prompt
```

### 2. Response Quality

‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á:

```python
def enhance_response(self, raw_response: str, results: Dict) -> str:
    """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á"""
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° Emoji ‡∏ï‡∏≤‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå
    if 'face' in results:
        emotion = results['face'].get('emotion', 'neutral')
        emoji_map = {
            'happy': 'üòä',
            'sad': 'üò¢', 
            'angry': 'üò†',
            'surprised': 'üò≤',
            'neutral': 'üòä'
        }
        raw_response = f"{emoji_map.get(emotion, 'üòä')} {raw_response}"
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß
    if len(raw_response) > 100:
        raw_response = raw_response[:97] + "..."
    
    return raw_response
```

### 3. Error Handling

‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î:

```python
async def safe_generate_response(self, results: Dict) -> Dict:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏° Error Handling"""
    
    max_retries = 3
    
    for attempt in range(max_retries):
        try:
            return await self.generate_response(results)
            
        except Exception as e:
            print(f"‚ùå Attempt {attempt + 1} failed: {e}")
            
            if attempt == max_retries - 1:
                return {
                    'success': False,
                    'response': self._get_fallback_response(results),
                    'error': 'LLM service unavailable'
                }
            
            await asyncio.sleep(1)  # ‡∏£‡∏≠ 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
```

---

## üö® ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: API Key ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

```bash
‚ùå Error: Invalid API Key
```

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**
1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ API Key ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå `.env` ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå `backend/`
3. ‡∏£‡∏µ‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó server: `python app.py`

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: LLM ‡∏ï‡∏≠‡∏ö‡∏ä‡πâ‡∏≤

```bash
‚ö†Ô∏è Response time > 5 seconds
```

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**
1. ‡πÉ‡∏ä‡πâ `gemini-pro` ‡πÅ‡∏ó‡∏ô `gpt-4`
2. ‡∏•‡∏î `max_tokens` ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
3. ‡πÄ‡∏û‡∏¥‡πà‡∏° timeout handling

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: ‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**
1. ‡πÄ‡∏û‡∏¥‡πà‡∏° "‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô" ‡πÉ‡∏ô Prompt
2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ `LANGUAGE=th` ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå `.env`
3. ‡πÉ‡∏ä‡πâ Gemini (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ GPT)

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏ï‡πá‡∏°

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**
1. ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
2. ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î context window
3. ‡πÉ‡∏ä‡πâ model ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤

---

## üìä ‡∏Å‡∏≤‡∏£ Monitor ‡πÅ‡∏•‡∏∞ Analytics

### 1. Logging

‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def generate_response(self, results: Dict) -> Dict:
    start_time = time.time()
    
    try:
        # ... ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ...
        
        response_time = time.time() - start_time
        logger.info(f"LLM Response time: {response_time:.2f}s")
        
        return result
        
    except Exception as e:
        logger.error(f"LLM Error: {e}")
        raise
```

### 2. Usage Tracking

‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:

```python
class UsageTracker:
    def __init__(self):
        self.requests = 0
        self.successful_requests = 0
        self.total_tokens = 0
    
    def track_request(self, tokens: int, success: bool):
        self.requests += 1
        self.total_tokens += tokens
        if success:
            self.successful_requests += 1
    
    def get_stats(self):
        success_rate = self.successful_requests / self.requests * 100
        return {
            'total_requests': self.requests,
            'success_rate': f'{success_rate:.1f}%',
            'total_tokens': self.total_tokens,
            'avg_tokens': self.total_tokens / self.requests
        }
```

---

## üìû ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ó‡∏µ‡∏°

**‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤?** ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠:

- Person 1 & 2: ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Hand Models
- Person 3: ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Face API
- ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡∏°

**‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏î‡∏™‡∏≠‡∏ö:** `llm_backend_test.ipynb` - ‡πÉ‡∏ä‡πâ‡∏ó‡∏î‡∏™‡∏≠‡∏ö LLM ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏™‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏£‡∏¥‡∏á

---

## üéâ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

‡πÄ‡∏°‡∏∑‡πà‡∏≠ LLM Backend ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß:

1. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö API Endpoints** - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏∏‡∏Å endpoint ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
2. **‡∏ú‡∏™‡∏≤‡∏ô Frontend** - ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Frontend ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
3. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏ß‡∏°** - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö AI Models ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
4. **Deploy Production** - ‡∏ô‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô Server ‡∏à‡∏£‡∏¥‡∏á

**üåü ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: LLM Backend ‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥**
