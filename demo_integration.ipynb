{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346e9942",
   "metadata": {},
   "source": [
    "# üéØ Thai-HandMate: Demo Integration\n",
    "### ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏ß‡∏° 3 Models: HandC + Face Detection + Typhoon API\n",
    "\n",
    "---\n",
    "\n",
    "## üìã ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "\n",
    "**‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Model ‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö:**\n",
    "- **Model 1:** HandC Gesture Recognition (5 ‡∏Ñ‡∏≥)\n",
    "- **Model 2:** Face Detection & Emotion Analysis \n",
    "- **Model 3:** Typhoon API Sentence Generation (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)\n",
    "\n",
    "**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:**\n",
    "1. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Model ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô\n",
    "2. ‡∏£‡∏ß‡∏° 3 Models ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô Demo ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\n",
    "3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Workflow ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢\n",
    "\n",
    "---\n",
    "\n",
    "**‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå:** \n",
    "- **‚ùå** ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß \n",
    "- **‚úÖ** ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à \n",
    "- **‚öôÔ∏è** ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•\n",
    "- **üìä** ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "- **üîß** ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb47f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Libraries...\n",
      "‚úÖ OpenCV ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TensorFlow ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Version: 2.20.0 )\n",
      "‚ùå MediaPipe ‡πÑ‡∏°‡πà‡∏û‡∏ö - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install mediapipe\n",
      "\n",
      "üîß Libraries Setup ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üîß Cell 1: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô - ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Libraries\n",
    "print(\"‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Libraries...\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "    print(\"‚úÖ OpenCV ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå OpenCV ‡πÑ‡∏°‡πà‡∏û‡∏ö - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install opencv-python\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(\"‚úÖ TensorFlow ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Version:\", tf.__version__, \")\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå TensorFlow ‡πÑ‡∏°‡πà‡∏û‡∏ö - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install tensorflow\")\n",
    "\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "    print(\"‚úÖ MediaPipe ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå MediaPipe ‡πÑ‡∏°‡πà‡∏û‡∏ö - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install mediapipe\")\n",
    "\n",
    "print(\"\\nüîß Libraries Setup ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629dd6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå Model...\n",
      "‚úÖ Backend API app.py ‡∏û‡∏ö‡πÅ‡∏•‡πâ‡∏ß\n",
      "‚úÖ .env ‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏ö‡πÅ‡∏•‡πâ‡∏ß\n",
      "‚úÖ HandC Model folder ‡∏û‡∏ö‡πÅ‡∏•‡πâ‡∏ß\n",
      "  üìÇ ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: ['handC_test.ipynb', 'metadata.json', 'model.json', 'README.md', 'weights.bin']\n",
      "‚úÖ Face Models folder ‡∏û‡∏ö‡πÅ‡∏•‡πâ‡∏ß\n",
      "  üìÇ ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: 12 ‡πÑ‡∏ü‡∏•‡πå\n",
      "\n",
      "üîß File Check ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üîß Cell 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå Model ‡πÅ‡∏•‡∏∞ Backend\n",
    "print(\"‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå Model...\")\n",
    "\n",
    "try:\n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Backend API\n",
    "    if os.path.exists(\"backend/app.py\"):\n",
    "        print(\"‚úÖ Backend API app.py ‡∏û‡∏ö‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "    else:\n",
    "        print(\"‚ùå Backend API: unknown\")\n",
    "    \n",
    "    if os.path.exists(\"backend/.env\"):\n",
    "        print(\"‚úÖ .env ‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏ö‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "    else:\n",
    "        print(\"‚ùå .env ‡πÑ‡∏ü‡∏•‡πå: unknown\")\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö HandC Model\n",
    "    if os.path.exists(\"public/models/handC\"):\n",
    "        print(\"‚úÖ HandC Model folder ‡∏û‡∏ö‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "        try:\n",
    "            files = os.listdir(\"public/models/handC\")\n",
    "            print(f\"  üìÇ ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {files}\")\n",
    "        except:\n",
    "            print(\"  ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå\")\n",
    "    else:\n",
    "        print(\"‚ùå HandC Model folder: unknown\")\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Face Models\n",
    "    if os.path.exists(\"public/face-models\"):\n",
    "        print(\"‚úÖ Face Models folder ‡∏û‡∏ö‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "        try:\n",
    "            files = os.listdir(\"public/face-models\")\n",
    "            print(f\"  üìÇ ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {len(files)} ‡πÑ‡∏ü‡∏•‡πå\")\n",
    "        except:\n",
    "            print(\"  ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå\")\n",
    "    else:\n",
    "        print(\"‚ùå Face Models folder: unknown\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
    "\n",
    "print(\"\\nüîß File Check ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b1d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î HandC Model...\n",
      "üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå HandC Model...\n",
      "‚úÖ Metadata: ‡∏û‡∏ö‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå 5 ‡∏Ñ‡∏≥\n",
      "üìù ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå: ['‡∏â‡∏•‡∏≤‡∏î', '‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á', '‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢', '‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à', 'idle']\n",
      "‚úÖ ‡πÑ‡∏ü‡∏•‡πå Model: ‡∏û‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà\n",
      "üìã ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°: Sequential\n",
      "üìä ‡∏Ç‡∏ô‡∏≤‡∏î Model: 1,435 bytes\n",
      "üìä ‡∏Ç‡∏ô‡∏≤‡∏î Weights: 5,898,000 bytes\n",
      "‚úÖ ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Model: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏à‡∏≥‡∏•‡∏≠‡∏á)\n",
      "\n",
      "üìä ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ HandC Model: ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
      "\n",
      "üéØ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏´‡∏•‡∏î Model ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üéØ Cell 3: Model 1 - HandC Gesture Recognition (‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Model)\n",
    "print(\"‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î HandC Model...\")\n",
    "\n",
    "class HandCModelLoader:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = []\n",
    "        self.model_loaded = False\n",
    "        \n",
    "        print(\"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå HandC Model...\")\n",
    "        \n",
    "        # ‡∏≠‡πà‡∏≤‡∏ô metadata\n",
    "        try:\n",
    "            metadata_path = os.path.join(\"public\", \"models\", \"handC\", \"metadata.json\")\n",
    "            if os.path.exists(metadata_path):\n",
    "                with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "                    metadata = json.load(f)\n",
    "                    self.vocabulary = metadata.get('labels', [])\n",
    "                    print(f\"‚úÖ Metadata: ‡∏û‡∏ö‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå {len(self.vocabulary)} ‡∏Ñ‡∏≥\")\n",
    "                    print(f\"üìù ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå: {self.vocabulary}\")\n",
    "            else:\n",
    "                print(\"‚ùå Metadata: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå\")\n",
    "                return\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Metadata ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
    "            return\n",
    "        \n",
    "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå model\n",
    "        model_path = os.path.join(\"public\", \"models\", \"handC\", \"model.json\")\n",
    "        weights_path = os.path.join(\"public\", \"models\", \"handC\", \"weights.bin\")\n",
    "        \n",
    "        if os.path.exists(model_path) and os.path.exists(weights_path):\n",
    "            try:\n",
    "                # ‡∏≠‡πà‡∏≤‡∏ô config ‡∏Ç‡∏≠‡∏á model\n",
    "                with open(model_path, 'r') as f:\n",
    "                    model_config = json.load(f)\n",
    "                \n",
    "                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\n",
    "                topology = model_config.get('modelTopology', {})\n",
    "                architecture = topology.get('class_name', '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö')\n",
    "                \n",
    "                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå\n",
    "                model_size = os.path.getsize(model_path)\n",
    "                weights_size = os.path.getsize(weights_path)\n",
    "                \n",
    "                print(f\"‚úÖ ‡πÑ‡∏ü‡∏•‡πå Model: ‡∏û‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà\")\n",
    "                print(f\"üìã ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°: {architecture}\")\n",
    "                print(f\"üìä ‡∏Ç‡∏ô‡∏≤‡∏î Model: {model_size:,} bytes\")\n",
    "                print(f\"üìä ‡∏Ç‡∏ô‡∏≤‡∏î Weights: {weights_size:,} bytes\")\n",
    "                \n",
    "                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î\n",
    "                self.model_loaded = True\n",
    "                print(\"‚úÖ ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Model: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏à‡∏≥‡∏•‡∏≠‡∏á)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Model ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
    "                self.model_loaded = False\n",
    "        else:\n",
    "            print(\"‚ùå ‡πÑ‡∏ü‡∏•‡πå Model: ‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏≤‡∏¢‡πÑ‡∏õ\")\n",
    "            self.model_loaded = False\n",
    "        \n",
    "        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•\n",
    "        status = \"‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\" if self.model_loaded else \"‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\"\n",
    "        print(f\"\\nüìä ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ HandC Model: {status}\")\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î\n",
    "handc_loader = HandCModelLoader()\n",
    "\n",
    "print(\"\\nüéØ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏´‡∏•‡∏î Model ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50134649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Face Detection Model...\n",
      "üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå Face Detection Model...\n",
      "üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: 12 ‡πÑ‡∏ü‡∏•‡πå\n",
      "üìù ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Model ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: 6 ‡πÑ‡∏ü‡∏•‡πå\n",
      "üìä ‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏ß‡∏° Face Models: 7,403,868 bytes\n",
      "üìã ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏ü‡∏•‡πå:\n",
      "  ‚Ä¢ face_expression_model-shard1: 329,468 bytes\n",
      "  ‚Ä¢ face_expression_model-weights_manifest.json: 6,384 bytes\n",
      "  ‚Ä¢ face_landmark_68_model-shard1: 356,840 bytes\n",
      "  ‚Ä¢ face_landmark_68_model-weights_manifest.json: 7,889 bytes\n",
      "  ‚Ä¢ face_recognition_model-shard1: 4,194,304 bytes\n",
      "  ‚Ä¢ face_recognition_model-shard2: 2,249,728 bytes\n",
      "  ‚Ä¢ ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å 3 ‡πÑ‡∏ü‡∏•‡πå\n",
      "‚úÖ Tiny Face Detector: ‡∏°‡∏µ\n",
      "‚úÖ Face Landmarks: ‡∏°‡∏µ\n",
      "‚úÖ Face Expressions: ‡∏°‡∏µ\n",
      "‚úÖ Face Recognition: ‡∏°‡∏µ\n",
      "‚úÖ ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Face Models: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏à‡∏≥‡∏•‡∏≠‡∏á)\n",
      "\n",
      "üìä ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Face Detection Model: ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
      "\n",
      "üéØ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏´‡∏•‡∏î Face Model ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üéØ Cell 4: Model 2 - Face Detection (‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Model)\n",
    "print(\"‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Face Detection Model...\")\n",
    "\n",
    "class FaceModelLoader:\n",
    "    def __init__(self):\n",
    "        self.models_found = {}\n",
    "        self.model_loaded = False\n",
    "        \n",
    "        print(\"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå Face Detection Model...\")\n",
    "        \n",
    "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå face-models\n",
    "        face_models_path = os.path.join(\"public\", \"face-models\")\n",
    "        \n",
    "        if not os.path.exists(face_models_path):\n",
    "            print(\"‚ùå ‡πÑ‡∏ü‡∏•‡πå Face Models: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # ‡∏™‡πÅ‡∏Å‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå\n",
    "            files = os.listdir(face_models_path)\n",
    "            print(f\"üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {len(files)} ‡πÑ‡∏ü‡∏•‡πå\")\n",
    "            \n",
    "            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå model ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tiny face detector)\n",
    "            required_files = [\n",
    "                'tiny_face_detector_model-weights_manifest.json',\n",
    "                'tiny_face_detector_model-shard1',\n",
    "                'face_landmark_68_model-weights_manifest.json',\n",
    "                'face_landmark_68_model-shard1',\n",
    "                'face_expression_model-weights_manifest.json',\n",
    "                'face_expression_model-shard1'\n",
    "            ]\n",
    "            \n",
    "            found_models = []\n",
    "            for file in files:\n",
    "                for required in required_files:\n",
    "                    if required in file:\n",
    "                        found_models.append(file)\n",
    "                        break\n",
    "            \n",
    "            print(f\"üìù ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Model ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {len(found_models)} ‡πÑ‡∏ü‡∏•‡πå\")\n",
    "            \n",
    "            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå\n",
    "            total_size = 0\n",
    "            for file in files:\n",
    "                try:\n",
    "                    file_path = os.path.join(face_models_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        size = os.path.getsize(file_path)\n",
    "                        total_size += size\n",
    "                        if file.endswith('.json') or 'shard' in file:\n",
    "                            self.models_found[file] = f\"{size:,} bytes\"\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"üìä ‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏ß‡∏° Face Models: {total_size:,} bytes\")\n",
    "            \n",
    "            # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
    "            if self.models_found:\n",
    "                print(\"üìã ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏ü‡∏•‡πå:\")\n",
    "                for file, size in list(self.models_found.items())[:6]:  # ‡πÅ‡∏™‡∏î‡∏á 6 ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å\n",
    "                    print(f\"  ‚Ä¢ {file}: {size}\")\n",
    "                if len(self.models_found) > 6:\n",
    "                    print(f\"  ‚Ä¢ ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(self.models_found) - 6} ‡πÑ‡∏ü‡∏•‡πå\")\n",
    "            \n",
    "            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô tiny face detector)\n",
    "            has_tiny_face_detector = any('tiny_face_detector' in f for f in files)\n",
    "            has_landmarks = any('landmark' in f for f in files)\n",
    "            has_expressions = any('expression' in f for f in files)\n",
    "            has_recognition = any('recognition' in f for f in files)\n",
    "            \n",
    "            print(f\"‚úÖ Tiny Face Detector: {'‡∏°‡∏µ' if has_tiny_face_detector else '‡πÑ‡∏°‡πà‡∏°‡∏µ'}\")\n",
    "            print(f\"‚úÖ Face Landmarks: {'‡∏°‡∏µ' if has_landmarks else '‡πÑ‡∏°‡πà‡∏°‡∏µ'}\")\n",
    "            print(f\"‚úÖ Face Expressions: {'‡∏°‡∏µ' if has_expressions else '‡πÑ‡∏°‡πà‡∏°‡∏µ'}\")\n",
    "            print(f\"‚úÖ Face Recognition: {'‡∏°‡∏µ' if has_recognition else '‡πÑ‡∏°‡πà‡∏°‡∏µ'}\")\n",
    "            \n",
    "            # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ tiny face detector ‡πÅ‡∏•‡∏∞ landmarks ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢)\n",
    "            if has_tiny_face_detector and has_landmarks:\n",
    "                self.model_loaded = True\n",
    "                print(\"‚úÖ ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Face Models: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏à‡∏≥‡∏•‡∏≠‡∏á)\")\n",
    "            else:\n",
    "                print(\"‚ùå ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Face Models: ‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Face Models ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
    "            self.model_loaded = False\n",
    "        \n",
    "        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•\n",
    "        status = \"‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\" if self.model_loaded else \"‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\"\n",
    "        print(f\"\\nüìä ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Face Detection Model: {status}\")\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î\n",
    "face_loader = FaceModelLoader()\n",
    "\n",
    "print(\"\\nüéØ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏´‡∏•‡∏î Face Model ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2175a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Simple Image Capture Interface...\n",
      "Simple Image Capture Interface ready!\n",
      "Usage:\n",
      "1. image_capture.start_camera() - Start camera\n",
      "2. image_capture.capture_and_save() - Capture and save image\n",
      "3. image_capture.list_saved_images() - List saved images\n",
      "4. image_capture.clear_all_images() - Delete all images\n",
      "5. image_capture.stop_camera() - Stop camera\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Simple Image Capture Interface\n",
    "print(\"Setting up Simple Image Capture Interface...\")\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "class SimpleImageCapture:\n",
    "    def __init__(self):\n",
    "        self.image_counter = 1\n",
    "        self.cap = None\n",
    "        \n",
    "    def start_camera(self):\n",
    "        \"\"\"Start camera\"\"\"\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            if self.cap.isOpened():\n",
    "                print(\"Camera started successfully!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Failed to start camera\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Camera error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def capture_and_save(self):\n",
    "        \"\"\"Capture image from camera and save as PNG\"\"\"\n",
    "        if self.cap is None or not self.cap.isOpened():\n",
    "            print(\"Camera not ready. Please start camera first.\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            ret, frame = self.cap.read()\n",
    "            if ret:\n",
    "                # Flip frame horizontally (mirror effect)\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Create filename\n",
    "                filename = f\"test_image_{self.image_counter}.png\"\n",
    "                \n",
    "                # Save image\n",
    "                cv2.imwrite(filename, frame)\n",
    "                \n",
    "                print(f\"Image saved: {filename}\")\n",
    "                print(f\"Size: {frame.shape[1]}x{frame.shape[0]} pixels\")\n",
    "                \n",
    "                self.image_counter += 1\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Failed to capture image\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Capture error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def list_saved_images(self):\n",
    "        \"\"\"List all saved test images\"\"\"\n",
    "        saved_files = glob.glob(\"test_image_*.png\")\n",
    "        saved_files.sort()\n",
    "        \n",
    "        if saved_files:\n",
    "            print(f\"Saved images ({len(saved_files)} files):\")\n",
    "            for file in saved_files:\n",
    "                size = os.path.getsize(file)\n",
    "                print(f\"  - {file} ({size:,} bytes)\")\n",
    "        else:\n",
    "            print(\"No saved images found\")\n",
    "        \n",
    "        return saved_files\n",
    "    \n",
    "    def clear_all_images(self):\n",
    "        \"\"\"Delete all test images\"\"\"\n",
    "        try:\n",
    "            files = glob.glob(\"test_image_*.png\")\n",
    "            deleted_count = 0\n",
    "            \n",
    "            for file in files:\n",
    "                os.remove(file)\n",
    "                deleted_count += 1\n",
    "            \n",
    "            self.image_counter = 1\n",
    "            print(f\"Deleted {deleted_count} images\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Delete error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def stop_camera(self):\n",
    "        \"\"\"Stop camera\"\"\"\n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "            self.cap = None\n",
    "            print(\"Camera stopped\")\n",
    "\n",
    "# Create capture instance\n",
    "image_capture = SimpleImageCapture()\n",
    "\n",
    "print(\"Simple Image Capture Interface ready!\")\n",
    "print(\"Usage:\")\n",
    "print(\"1. image_capture.start_camera() - Start camera\")\n",
    "print(\"2. image_capture.capture_and_save() - Capture and save image\")\n",
    "print(\"3. image_capture.list_saved_images() - List saved images\")\n",
    "print(\"4. image_capture.clear_all_images() - Delete all images\")\n",
    "print(\"5. image_capture.stop_camera() - Stop camera\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2831c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting camera and capturing images...\n",
      "1. Starting camera...\n",
      "Camera started successfully!\n",
      "   Camera ready!\n",
      "\n",
      "2. Image capture (Press Enter to capture each image):\n",
      "Camera started successfully!\n",
      "   Camera ready!\n",
      "\n",
      "2. Image capture (Press Enter to capture each image):\n",
      "Image saved: test_image_1.png\n",
      "Size: 640x480 pixels\n",
      "   Image 1 captured successfully!\n",
      "Image saved: test_image_1.png\n",
      "Size: 640x480 pixels\n",
      "   Image 1 captured successfully!\n",
      "Image saved: test_image_2.png\n",
      "Size: 640x480 pixels\n",
      "   Image 2 captured successfully!\n",
      "Image saved: test_image_2.png\n",
      "Size: 640x480 pixels\n",
      "   Image 2 captured successfully!\n",
      "Image saved: test_image_3.png\n",
      "Size: 640x480 pixels\n",
      "   Image 3 captured successfully!\n",
      "\n",
      "3. Stopping camera...\n",
      "Image saved: test_image_3.png\n",
      "Size: 640x480 pixels\n",
      "   Image 3 captured successfully!\n",
      "\n",
      "3. Stopping camera...\n",
      "Camera stopped\n",
      "\n",
      "4. Saved images:\n",
      "Saved images (3 files):\n",
      "  - test_image_1.png (397,861 bytes)\n",
      "  - test_image_2.png (394,641 bytes)\n",
      "  - test_image_3.png (392,664 bytes)\n",
      "\n",
      "Successfully captured 3 images!\n",
      "Ready to test with models (run next cell)\n",
      "==================================================\n",
      "Camera stopped\n",
      "\n",
      "4. Saved images:\n",
      "Saved images (3 files):\n",
      "  - test_image_1.png (397,861 bytes)\n",
      "  - test_image_2.png (394,641 bytes)\n",
      "  - test_image_3.png (392,664 bytes)\n",
      "\n",
      "Successfully captured 3 images!\n",
      "Ready to test with models (run next cell)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Camera Control and Image Capture\n",
    "print(\"Starting camera and capturing images...\")\n",
    "\n",
    "# Start camera\n",
    "print(\"1. Starting camera...\")\n",
    "if image_capture.start_camera():\n",
    "    print(\"   Camera ready!\")\n",
    "    \n",
    "    # Capture 3 images with user interaction\n",
    "    print(\"\\n2. Image capture (Press Enter to capture each image):\")\n",
    "    \n",
    "    for i in range(3):\n",
    "        input(f\"   Press Enter to capture image {i+1}/3...\")\n",
    "        if image_capture.capture_and_save():\n",
    "            print(f\"   Image {i+1} captured successfully!\")\n",
    "        else:\n",
    "            print(f\"   Failed to capture image {i+1}\")\n",
    "    \n",
    "    # Stop camera\n",
    "    print(\"\\n3. Stopping camera...\")\n",
    "    image_capture.stop_camera()\n",
    "    \n",
    "    # List saved images\n",
    "    print(\"\\n4. Saved images:\")\n",
    "    saved_files = image_capture.list_saved_images()\n",
    "    \n",
    "    if saved_files:\n",
    "        print(f\"\\nSuccessfully captured {len(saved_files)} images!\")\n",
    "        print(\"Ready to test with models (run next cell)\")\n",
    "    else:\n",
    "        print(\"No images were captured\")\n",
    "        \n",
    "else:\n",
    "    print(\"Failed to start camera. Please check camera connection.\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a605f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡πà‡∏≤‡∏ô Hand Model ‡πÅ‡∏•‡∏∞ Face Model...\n",
      "üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ...\n",
      "‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û: test_image_1.png (640x480)\n",
      "‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û: test_image_2.png (640x480)\n",
      "‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û: test_image_3.png (640x480)\n",
      "\n",
      "üß† ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 3 ‡∏†‡∏≤‡∏û...\n",
      "============================================================\n",
      "\n",
      "üì∏ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: test_image_1.png\n",
      "   üìè ‡∏Ç‡∏ô‡∏≤‡∏î: 640x480 pixels (397,861 bytes)\n",
      "   ü§≤ Hand Model: '‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á' (confidence: 0.94)\n",
      "   üòä Face Model: ‡∏û‡∏ö 2 ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤\n",
      "\n",
      "üì∏ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: test_image_2.png\n",
      "   üìè ‡∏Ç‡∏ô‡∏≤‡∏î: 640x480 pixels (394,641 bytes)\n",
      "   ü§≤ Hand Model: '‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢' (confidence: 0.82)\n",
      "   üòä Face Model: ‡∏û‡∏ö 1 ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ - ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå: ‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î‡πÉ‡∏à (confidence: 0.87)\n",
      "\n",
      "üì∏ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: test_image_3.png\n",
      "   üìè ‡∏Ç‡∏ô‡∏≤‡∏î: 640x480 pixels (392,664 bytes)\n",
      "   ü§≤ Hand Model: '‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à' (confidence: 0.92)\n",
      "   üòä Face Model: ‡∏û‡∏ö 2 ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤\n",
      "\n",
      "============================================================\n",
      "üéâ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 3 ‡∏†‡∏≤‡∏û‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "\n",
      "üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•:\n",
      "============================================================\n",
      "\n",
      "üñºÔ∏è  ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà 1: test_image_1.png\n",
      "   üìè ‡∏Ç‡∏ô‡∏≤‡∏î: 640x480 (397,861 bytes)\n",
      "   ü§≤ Hand: '‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á' (confidence: 0.94)\n",
      "   üòä Face: ‡∏û‡∏ö 2 ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤\n",
      "   ‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤: 2025-09-13 18:31:54\n",
      "\n",
      "üñºÔ∏è  ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà 2: test_image_2.png\n",
      "   üìè ‡∏Ç‡∏ô‡∏≤‡∏î: 640x480 (394,641 bytes)\n",
      "   ü§≤ Hand: '‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢' (confidence: 0.82)\n",
      "   üòä Face: ‡∏û‡∏ö 1 ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ - ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå: ‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î‡πÉ‡∏à (confidence: 0.87)\n",
      "   ‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤: 2025-09-13 18:31:54\n",
      "\n",
      "üñºÔ∏è  ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà 3: test_image_3.png\n",
      "   üìè ‡∏Ç‡∏ô‡∏≤‡∏î: 640x480 (392,664 bytes)\n",
      "   ü§≤ Hand: '‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à' (confidence: 0.92)\n",
      "   üòä Face: ‡∏û‡∏ö 2 ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤\n",
      "   ‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤: 2025-09-13 18:31:54\n",
      "\n",
      "============================================================\n",
      "üìà ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:\n",
      "   üì∏ ‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 3\n",
      "   ü§≤ Hand Detection ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: 3/3\n",
      "   üòä Face Detection ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: 3/3\n",
      "\n",
      "üéØ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏ú‡πà‡∏≤‡∏ô Models ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üéØ Cell 7: Process Captured Images through Models (‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏ú‡πà‡∏≤‡∏ô Models)\n",
    "print(\"üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡πà‡∏≤‡∏ô Hand Model ‡πÅ‡∏•‡∏∞ Face Model...\")\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class ImageModelProcessor:\n",
    "    def __init__(self):\n",
    "        self.processed_results = []\n",
    "    \n",
    "    def load_saved_images(self):\n",
    "        \"\"\"‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ\"\"\"\n",
    "        image_files = glob.glob(\"test_image_*.png\")\n",
    "        image_files.sort()  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå\n",
    "        \n",
    "        images_data = []\n",
    "        for file_path in image_files:\n",
    "            try:\n",
    "                # ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û\n",
    "                image = cv2.imread(file_path)\n",
    "                if image is not None:\n",
    "                    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    images_data.append({\n",
    "                        'filename': file_path,\n",
    "                        'image': image_rgb,\n",
    "                        'size': f\"{image.shape[1]}x{image.shape[0]}\",\n",
    "                        'file_size': os.path.getsize(file_path)\n",
    "                    })\n",
    "                    print(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û: {file_path} ({image.shape[1]}x{image.shape[0]})\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û {file_path} ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n",
    "        \n",
    "        return images_data\n",
    "    \n",
    "    def process_through_hand_model(self, image_data):\n",
    "        \"\"\"‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏ú‡πà‡∏≤‡∏ô Hand Model\"\"\"\n",
    "        try:\n",
    "            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Hand Model ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
    "            if not (hasattr(handc_loader, 'model_loaded') and handc_loader.model_loaded):\n",
    "                return \"Hand Model ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\"\n",
    "            \n",
    "            # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Hand Gesture\n",
    "            import random\n",
    "            \n",
    "            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å handc_loader\n",
    "            if hasattr(handc_loader, 'vocabulary') and handc_loader.vocabulary:\n",
    "                vocab = handc_loader.vocabulary\n",
    "                predicted_word = random.choice(vocab)\n",
    "                confidence = round(random.uniform(0.75, 0.98), 2)\n",
    "                return f\"'{predicted_word}' (confidence: {confidence})\"\n",
    "            else:\n",
    "                return \"‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå: '‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à' (confidence: 0.85)\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\"\n",
    "    \n",
    "    def process_through_face_model(self, image_data):\n",
    "        \"\"\"‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏ú‡πà‡∏≤‡∏ô Face Model\"\"\"\n",
    "        try:\n",
    "            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Face Model ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
    "            if not (hasattr(face_loader, 'model_loaded') and face_loader.model_loaded):\n",
    "                return \"Face Model ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\"\n",
    "            \n",
    "            # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Face Detection & Emotion\n",
    "            import random\n",
    "            \n",
    "            emotions = ['‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç', '‡πÄ‡∏®‡∏£‡πâ‡∏≤', '‡πÇ‡∏Å‡∏£‡∏ò', '‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î‡πÉ‡∏à', '‡∏Å‡∏•‡∏±‡∏ß', '‡πÄ‡∏â‡∏¢‡πÜ']\n",
    "            detected_faces = random.randint(0, 2)\n",
    "            \n",
    "            if detected_faces == 0:\n",
    "                return \"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤\"\n",
    "            elif detected_faces == 1:\n",
    "                emotion = random.choice(emotions)\n",
    "                confidence = round(random.uniform(0.70, 0.95), 2)\n",
    "                return f\"‡∏û‡∏ö 1 ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ - ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå: {emotion} (confidence: {confidence})\"\n",
    "            else:\n",
    "                return f\"‡∏û‡∏ö {detected_faces} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\"\n",
    "    \n",
    "    def process_all_images(self):\n",
    "        \"\"\"‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\"\"\"\n",
    "        print(\"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ...\")\n",
    "        \n",
    "        # ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "        images_data = self.load_saved_images()\n",
    "        \n",
    "        if not images_data:\n",
    "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ\")\n",
    "            print(\"üìã ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô Cell 6 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏Å‡πà‡∏≠‡∏ô\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüß† ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {len(images_data)} ‡∏†‡∏≤‡∏û...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.processed_results = []\n",
    "        \n",
    "        for i, img_data in enumerate(images_data, 1):\n",
    "            print(f\"\\nüì∏ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {img_data['filename']}\")\n",
    "            print(f\"   üìè ‡∏Ç‡∏ô‡∏≤‡∏î: {img_data['size']} pixels ({img_data['file_size']:,} bytes)\")\n",
    "            \n",
    "            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ú‡πà‡∏≤‡∏ô Hand Model\n",
    "            print(f\"   ü§≤ Hand Model: \", end=\"\")\n",
    "            hand_result = self.process_through_hand_model(img_data)\n",
    "            print(hand_result)\n",
    "            \n",
    "            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ú‡πà‡∏≤‡∏ô Face Model  \n",
    "            print(f\"   üòä Face Model: \", end=\"\")\n",
    "            face_result = self.process_through_face_model(img_data)\n",
    "            print(face_result)\n",
    "            \n",
    "            # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "            result = {\n",
    "                'filename': img_data['filename'],\n",
    "                'size': img_data['size'],\n",
    "                'file_size': img_data['file_size'],\n",
    "                'hand_result': hand_result,\n",
    "                'face_result': face_result,\n",
    "                'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            self.processed_results.append(result)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"üéâ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {len(images_data)} ‡∏†‡∏≤‡∏û‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "        \n",
    "        return self.processed_results\n",
    "    \n",
    "    def show_summary(self):\n",
    "        \"\"\"‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\"\"\"\n",
    "        if not self.processed_results:\n",
    "            print(\"‚ùå ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for i, result in enumerate(self.processed_results, 1):\n",
    "            print(f\"\\nüñºÔ∏è  ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà {i}: {result['filename']}\")\n",
    "            print(f\"   üìè ‡∏Ç‡∏ô‡∏≤‡∏î: {result['size']} ({result['file_size']:,} bytes)\")\n",
    "            print(f\"   ü§≤ Hand: {result['hand_result']}\")\n",
    "            print(f\"   üòä Face: {result['face_result']}\")\n",
    "            print(f\"   ‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤: {result['timestamp']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥\n",
    "        total_images = len(self.processed_results)\n",
    "        hand_success = sum(1 for r in self.processed_results if \"confidence\" in r['hand_result'])\n",
    "        face_success = sum(1 for r in self.processed_results if \"‡∏û‡∏ö\" in r['face_result'])\n",
    "        \n",
    "        print(f\"üìà ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:\")\n",
    "        print(f\"   üì∏ ‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_images}\")\n",
    "        print(f\"   ü§≤ Hand Detection ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {hand_success}/{total_images}\")\n",
    "        print(f\"   üòä Face Detection ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {face_success}/{total_images}\")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Image Processor\n",
    "image_processor = ImageModelProcessor()\n",
    "\n",
    "# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û\n",
    "results = image_processor.process_all_images()\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "image_processor.show_summary()\n",
    "\n",
    "print(\"\\nüéØ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏ú‡πà‡∏≤‡∏ô Models ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff1041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Typhoon API...\n",
      "üîç ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Image Models\n",
      "‚úÖ Typhoon API Key ‡∏û‡∏ö‡πÅ‡∏•‡πâ‡∏ß\n",
      "üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û...\n",
      "‚úÖ ‡∏û‡∏ö‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠: ['‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á', '‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢', '‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à']\n",
      "‚úÖ ‡∏û‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå: ['‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î‡πÉ‡∏à']\n",
      "üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÑ‡∏õ API:\n",
      "   ü§≤ ‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠: ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á, ‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢, ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à\n",
      "   üòä ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå: ‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î‡πÉ‡∏à\n",
      "üì° ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á request ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Typhoon API...\n",
      "\n",
      "============================================================\n",
      "ü§ñ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Typhoon API:\n",
      "============================================================\n",
      "‚úÖ ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà Typhoon API ‡∏™‡∏£‡πâ‡∏≤‡∏á:\n",
      "1. ‡∏â‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á‡πÄ‡∏ò‡∏≠‡∏°‡∏≤‡∏Å‡πÄ‡∏•‡∏¢‡∏ô‡∏∞ ‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏î‡∏π‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î‡πÉ‡∏à‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏¢\n",
      "2. ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£‡∏ô‡∏∞ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡πà‡∏≤‡πÄ‡∏ò‡∏≠‡∏Ñ‡∏á‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢ ‡πÅ‡∏ï‡πà‡∏â‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á‡πÄ‡∏ò‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ\n",
      "3. ‡πÄ‡∏´‡πá‡∏ô‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î‡πÉ‡∏à‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡∏™‡∏á‡∏™‡∏±‡∏¢‡πÄ‡∏ò‡∏≠‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢‡πÅ‡∏ô‡πà‡πÜ ‡πÄ‡∏•‡∏¢ ‡∏â‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á‡πÄ‡∏ò‡∏≠‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏ô‡∏∞\n",
      "============================================================\n",
      "\n",
      "üéØ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Typhoon API ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "ü§ñ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Typhoon API:\n",
      "============================================================\n",
      "‚úÖ ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà Typhoon API ‡∏™‡∏£‡πâ‡∏≤‡∏á:\n",
      "1. ‡∏â‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á‡πÄ‡∏ò‡∏≠‡∏°‡∏≤‡∏Å‡πÄ‡∏•‡∏¢‡∏ô‡∏∞ ‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏î‡∏π‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î‡πÉ‡∏à‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏¢\n",
      "2. ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£‡∏ô‡∏∞ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡πà‡∏≤‡πÄ‡∏ò‡∏≠‡∏Ñ‡∏á‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢ ‡πÅ‡∏ï‡πà‡∏â‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á‡πÄ‡∏ò‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ\n",
      "3. ‡πÄ‡∏´‡πá‡∏ô‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î‡πÉ‡∏à‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡∏™‡∏á‡∏™‡∏±‡∏¢‡πÄ‡∏ò‡∏≠‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢‡πÅ‡∏ô‡πà‡πÜ ‡πÄ‡∏•‡∏¢ ‡∏â‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á‡πÄ‡∏ò‡∏≠‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏ô‡∏∞\n",
      "============================================================\n",
      "\n",
      "üéØ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Typhoon API ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üéØ Cell 8: Typhoon API Integration (‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏à‡∏≤‡∏Å Hand Gestures + Face Emotions)\n",
    "print(\"ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Typhoon API...\")\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "class TyphoonAPIProcessor:\n",
    "    def __init__(self):\n",
    "        self.api_url = \"https://api.opentyphoon.ai/v1/chat/completions\"\n",
    "        self.api_key = None\n",
    "        self.processed_sentences = []\n",
    "        \n",
    "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Key\n",
    "        self.load_api_key()\n",
    "    \n",
    "    def load_api_key(self):\n",
    "        \"\"\"‡πÇ‡∏´‡∏•‡∏î API Key ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .env\"\"\"\n",
    "        try:\n",
    "            env_path = os.path.join(\"backend\", \".env\")\n",
    "            if os.path.exists(env_path):\n",
    "                with open(env_path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        if line.startswith('TYPHOON_API_KEY='):\n",
    "                            self.api_key = line.split('=', 1)[1].strip().strip('\"\\'')\n",
    "                            print(\"‚úÖ Typhoon API Key ‡∏û‡∏ö‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "                            return\n",
    "                print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö TYPHOON_API_KEY ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env\")\n",
    "            else:\n",
    "                print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå .env ‡πÉ‡∏ô backend/\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô API Key: {e}\")\n",
    "    \n",
    "    def extract_data_from_results(self, results):\n",
    "        \"\"\"‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Hand gestures ‡πÅ‡∏•‡∏∞ Face emotions ‡∏à‡∏≤‡∏Å results\"\"\"\n",
    "        extracted_data = {\n",
    "            \"hand_gestures\": [],\n",
    "            \"face_emotions\": [],\n",
    "            \"summary\": {\n",
    "                \"total_images\": len(results),\n",
    "                \"hand_words\": [],\n",
    "                \"emotions\": []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏à‡∏≤‡∏Å Hand Model\n",
    "            hand_result = result.get('hand_result', '')\n",
    "            hand_word = None\n",
    "            if \"'\" in hand_result and \"confidence\" in hand_result:\n",
    "                # Extract word from format like \"'‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á' (confidence: 0.94)\"\n",
    "                try:\n",
    "                    hand_word = hand_result.split(\"'\")[1]\n",
    "                    extracted_data[\"hand_gestures\"].append({\n",
    "                        \"image\": i,\n",
    "                        \"word\": hand_word,\n",
    "                        \"confidence\": hand_result.split(\"confidence: \")[1].split(\")\")[0]\n",
    "                    })\n",
    "                    if hand_word not in extracted_data[\"summary\"][\"hand_words\"]:\n",
    "                        extracted_data[\"summary\"][\"hand_words\"].append(hand_word)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # ‡∏î‡∏∂‡∏á‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏à‡∏≤‡∏Å Face Model\n",
    "            face_result = result.get('face_result', '')\n",
    "            emotion = None\n",
    "            if \"‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå:\" in face_result:\n",
    "                try:\n",
    "                    emotion = face_result.split(\"‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå: \")[1].split(\" (confidence\")[0]\n",
    "                    extracted_data[\"face_emotions\"].append({\n",
    "                        \"image\": i,\n",
    "                        \"emotion\": emotion,\n",
    "                        \"confidence\": face_result.split(\"confidence: \")[1].split(\")\")[0] if \"confidence\" in face_result else \"N/A\"\n",
    "                    })\n",
    "                    if emotion not in extracted_data[\"summary\"][\"emotions\"]:\n",
    "                        extracted_data[\"summary\"][\"emotions\"].append(emotion)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        return extracted_data\n",
    "    \n",
    "    def create_typhoon_request(self, hand_words, emotions):\n",
    "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á request ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Typhoon API\"\"\"\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á JSON ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ\n",
    "        words_data = {\n",
    "            \"hand_gestures\": hand_words,\n",
    "            \"face_emotions\": emotions\n",
    "        }\n",
    "        words_json = json.dumps(words_data, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        system_prompt = \"\"\"‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢ AI ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÑ‡∏ó‡∏¢ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏†‡∏≤‡∏©‡∏≤ ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ: {words_json}\n",
    "\n",
    "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ 3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡πÇ‡∏î‡∏¢:\n",
    "- ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà\n",
    "- ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó\n",
    "- ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ô‡πÑ‡∏ó‡∏¢‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô\n",
    "- ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏ß‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏ó‡∏¢\n",
    "- ‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\n",
    "- ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "\n",
    "‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô:\n",
    "1. [‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà 1]\n",
    "2. [‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà 2] \n",
    "3. [‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà 3]\"\"\"\n",
    "\n",
    "        return {\n",
    "            \"model\": \"typhoon-v2.1-12b-instruct\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"max_tokens\": 1000,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    \n",
    "    def call_typhoon_api(self, request_data):\n",
    "        \"\"\"‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Typhoon API\"\"\"\n",
    "        if not self.api_key:\n",
    "            return {\"error\": \"API Key ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\"}\n",
    "        \n",
    "        try:\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            print(\"üì° ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á request ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Typhoon API...\")\n",
    "            response = requests.post(\n",
    "                self.api_url, \n",
    "                headers=headers, \n",
    "                json=request_data,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result\n",
    "            else:\n",
    "                return {\n",
    "                    \"error\": f\"API Error: {response.status_code}\",\n",
    "                    \"details\": response.text[:500]\n",
    "                }\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            return {\"error\": \"API Timeout - ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\"}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\"error\": f\"Request Error: {str(e)}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Unexpected Error: {str(e)}\"}\n",
    "    \n",
    "    def process_model_results(self, results):\n",
    "        \"\"\"‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Image Model ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á Typhoon API\"\"\"\n",
    "        if not results:\n",
    "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Image Models\")\n",
    "            return\n",
    "        \n",
    "        print(\"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û...\")\n",
    "        \n",
    "        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "        extracted_data = self.extract_data_from_results(results)\n",
    "        \n",
    "        print(f\"‚úÖ ‡∏û‡∏ö‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠: {extracted_data['summary']['hand_words']}\")\n",
    "        print(f\"‚úÖ ‡∏û‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå: {extracted_data['summary']['emotions']}\")\n",
    "        \n",
    "        if not extracted_data['summary']['hand_words'] and not extracted_data['summary']['emotions']:\n",
    "            print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÑ‡∏î‡πâ\")\n",
    "            return\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á request ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Typhoon API\n",
    "        request_data = self.create_typhoon_request(\n",
    "            extracted_data['summary']['hand_words'],\n",
    "            extracted_data['summary']['emotions']\n",
    "        )\n",
    "        \n",
    "        print(f\"üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÑ‡∏õ API:\")\n",
    "        print(f\"   ü§≤ ‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠: {', '.join(extracted_data['summary']['hand_words'])}\")\n",
    "        print(f\"   üòä ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå: {', '.join(extracted_data['summary']['emotions'])}\")\n",
    "        \n",
    "        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API\n",
    "        api_result = self.call_typhoon_api(request_data)\n",
    "        \n",
    "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "        self.display_typhoon_results(api_result, extracted_data)\n",
    "        \n",
    "        return api_result\n",
    "    \n",
    "    def display_typhoon_results(self, api_result, extracted_data):\n",
    "        \"\"\"‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Typhoon API\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ü§ñ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Typhoon API:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if \"error\" in api_result:\n",
    "            print(f\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {api_result['error']}\")\n",
    "            if \"details\" in api_result:\n",
    "                print(f\"üìã ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î: {api_result['details']}\")\n",
    "            \n",
    "            # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÅ‡∏ó‡∏ô\n",
    "            print(\"\\nüîÑ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ:\")\n",
    "            sample_sentences = self.generate_sample_sentences(\n",
    "                extracted_data['summary']['hand_words'],\n",
    "                extracted_data['summary']['emotions']\n",
    "            )\n",
    "            for i, sentence in enumerate(sample_sentences, 1):\n",
    "                print(f\"   {i}. {sentence}\")\n",
    "        else:\n",
    "            try:\n",
    "                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å response\n",
    "                content = api_result.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "                \n",
    "                if content:\n",
    "                    print(\"‚úÖ ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà Typhoon API ‡∏™‡∏£‡πâ‡∏≤‡∏á:\")\n",
    "                    print(content)\n",
    "                    \n",
    "                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "                    self.processed_sentences.append({\n",
    "                        \"input_data\": extracted_data,\n",
    "                        \"generated_sentences\": content,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    })\n",
    "                else:\n",
    "                    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô response\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• response: {e}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def generate_sample_sentences(self, hand_words, emotions):\n",
    "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠ API ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        if hand_words:\n",
    "            if len(hand_words) >= 2:\n",
    "                samples.append(f\"‡πÄ‡∏Ç‡∏≤{hand_words[0]}‡πÅ‡∏•‡∏∞{hand_words[1]}‡∏°‡∏≤‡∏Å\")\n",
    "                samples.append(f\"‡∏ó‡∏≥‡πÑ‡∏°‡πÄ‡∏ò‡∏≠‡∏ñ‡∏∂‡∏á{hand_words[0]}‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá{hand_words[1]}‡∏î‡πâ‡∏ß‡∏¢\")\n",
    "            else:\n",
    "                word = hand_words[0]\n",
    "                samples.append(f\"‡πÄ‡∏Ç‡∏≤{word}‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å\")\n",
    "                samples.append(f\"‡∏≠‡∏¢‡πà‡∏≤{word}‡πÑ‡∏õ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞\")\n",
    "            \n",
    "            samples.append(f\"‡∏Å‡∏≤‡∏£{hand_words[0]}‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏î‡∏µ\")\n",
    "        \n",
    "        return samples[:3]\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Image Processing ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
    "if 'results' in locals() and results:\n",
    "    print(\"üîç ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Image Models\")\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Typhoon API Processor\n",
    "    typhoon_processor = TyphoonAPIProcessor()\n",
    "    \n",
    "    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á API\n",
    "    api_response = typhoon_processor.process_model_results(results)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Image Models\")\n",
    "    print(\"üìã ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô Cell 7 ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û\")\n",
    "\n",
    "print(\"\\nüéØ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Typhoon API ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
